<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">DMN example application with Quarkus</title><link rel="alternate" href="http://www.mastertheboss.com/bpm/kogito/dmn-example-application-with-quarkus/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=dmn-example-application-with-quarkus" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/bpm/kogito/dmn-example-application-with-quarkus/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=dmn-example-application-with-quarkus</id><updated>2021-12-09T17:30:21Z</updated><content type="html">In our first tutorial about DMN – Getting started with Decision Models (DMN) – we have covered the foundation of Decision Models and the key components in it. In this follow up article we will design and create an example application which exposes a simple Decision Model through Quarkus REST Services. Designing the Decision Model ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Smart Scroll for BPMN/DMN Editors</title><link rel="alternate" href="https://blog.kie.org/2021/12/smart-scroll-for-bpmn-admn-editors.html" /><author><name>Wagner Lemos</name></author><id>https://blog.kie.org/2021/12/smart-scroll-for-bpmn-admn-editors.html</id><updated>2021-12-09T15:21:06Z</updated><content type="html">INTRODUCING SMART SCROLL Creating and managing BPMN/DMN resources can be a daunting task. Projects can come in many shapes and sizes and with varying levels of complexity. Consequently, being able to locate and identify nodes is of utmost importance. Focusing on easing the load of such demands, we introduce Smart Scroll for BPMN/DMN editors. Available for , this new feature enables scroll by dragging a canvas element close to a border. With it, node placement becomes a more natural task. USING SMART SCROLL To activate it, drag any canvas element (e.g., all nodes, bend points, and connection points) closer to the canvas border. Then, while still holding the mouse drag button, cease mouse movement. As a result, scrolling starts in the desired direction. Mind you, due to canvas limitations, scrolling beyond the origin limit (depicted by the dashed lines) is not possible. First to the right, then to the left – Smart Scroll activation No scrolling beyond canvas origin limit – Smart Scroll limits Canvas origin limits depiction in detail – Smart Scroll limits SCROLL BAR IMPROVEMENTS By itself, the Smart Scroll already proves valuable. But, to stand out and give that "just right" feeling, other changes were necessary. Next, we discuss them and show a side-by-side comparison of old vs. new to highlight the differences. The first change is real-time updates. In sum, there is no need to end element drag for the scrollbar to update. It happens as soon as there is drag movement. In the comparison below, pay attention to the scroll bar at the bottom. Drag end triggers scroll bar update – Scroll bar real-time update (Old Version) Drag movement triggering scroll bar update – Scroll bar real-time update (New Version) Next, we have canvas pan synchronization. In short, canvas pan updates the scroll bar. As a result, the scroll bar position and canvas pan are always in sync. Again focus on the scroll bar at the bottom to spot the difference. The scroll bar is not updated – Pan Synchronization (Old Version) The scroll bar updates in sync with the canvas pan – Pan Synchronization (New Version) Lastly, there is canvas size calculation. In brief, besides the existing elements, we also consider the visible area of the canvas. Due to this, there are no awkward jumps after dragging or scrolling. Noticeable jumps – Canvas size calculation (Old Version) Smooth updates without jumps – Canvas size calculation (New Version) CONCLUSION In conclusion, high cohesion while maintaining usability was the key goal here. We hope this will help users have a better experience using our editors. Make sure to check out our . You can stay up to date on our releases, new tools and have access to exceptional articles. Thanks for reading. See you next time. The post appeared first on .</content><dc:creator>Wagner Lemos</dc:creator></entry><entry><title type="html">Getting started with Decision Models (DMN)</title><link rel="alternate" href="http://www.mastertheboss.com/uncategorised/getting-started-with-decision-models-dmn/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=getting-started-with-decision-models-dmn" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/uncategorised/getting-started-with-decision-models-dmn/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=getting-started-with-decision-models-dmn</id><updated>2021-12-09T15:19:14Z</updated><content type="html">The Decision Model and Notation (DMN™) is a modelling language notation created by OMG for the specification of business decisions and business rules. In this tutorial we will have an overview of what is DMN and which are the key constructs of this language that you can use to model your Decisions that drive your ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Printf-style debugging using GDB, Part 3</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/09/printf-style-debugging-using-gdb-part-3" /><author><name>Kevin Buettner</name></author><id>227e9ca6-6710-47d9-824a-97204310bb9b</id><updated>2021-12-09T07:00:00Z</updated><published>2021-12-09T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome back to this series about using the &lt;a href="https://www.gnu.org/software/gdb/"&gt;GNU debugger&lt;/a&gt; (GDB) to print information in a way that is similar to using print statements in your code. The &lt;a href="https://developers.redhat.com/articles/2021/10/05/printf-style-debugging-using-gdb-part-1"&gt;first article&lt;/a&gt; introduced you to using GDB for printf-style debugging, and the &lt;a href="https://developers.redhat.com/articles/2021/10/13/printf-style-debugging-using-gdb-part-2"&gt;second article&lt;/a&gt; showed how to save commands and output. This final article demonstrates the power of GDB to interact with &lt;a href="https://developers.redhat.com/topics/c"&gt;C and C++&lt;/a&gt; functions and automate GDB behavior.&lt;/p&gt; &lt;h2&gt;Calling program-defined output routines&lt;/h2&gt; &lt;p&gt;Our example program contains a function named &lt;code&gt;print_tree&lt;/code&gt; that outputs a constructed tree. Suppose that you wish to use that function to examine the tree on which the &lt;code&gt;insert&lt;/code&gt; function will operate each time it is called. This can be done by setting a breakpoint on &lt;code&gt;insert&lt;/code&gt; and then specifying GDB commands to execute each time the breakpoint is hit. But before looking at how that's done, let's first look at how ordinary GDB breakpoints work.&lt;/p&gt; &lt;h3&gt;Setting a breakpoint&lt;/h3&gt; &lt;p&gt;A breakpoint can be set using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Set-Breaks.html#index-break"&gt;break&lt;/a&gt; command. We can use it to set a breakpoint at the start of the &lt;code&gt;insert&lt;/code&gt; function, as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) break insert Breakpoint 1 at 0x40127a: file tree.c, line 40. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you refer back to the source code from &lt;a href="https://developers.redhat.com/articles/2021/10/05/printf-style-debugging-using-gdb-part-1"&gt;Part 1&lt;/a&gt;, you'll see that line 40 is the first executable line of the function. If you run the program, GDB stops at the breakpoint, showing the values of the arguments to the function in addition to the line at which GDB has stopped:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) run Starting program: /home/kev/ctests/tree  Breakpoint 1, insert (tree=0x0, data=0x40203f "dog") at tree.c:40 40      if (tree == NULL) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are many interesting things you could do now, such as examining a stack trace via the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Backtrace.html#index-backtrace"&gt;backtrace&lt;/a&gt; command, or perhaps printing other values using GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Data.html#index-print"&gt;print&lt;/a&gt; command. However, I wish to demonstrate the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Calling.html#index-call"&gt;call&lt;/a&gt; and &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Continuing-and-Stepping.html#index-continue"&gt;continue&lt;/a&gt; commands that we'll use in the next section.&lt;/p&gt; &lt;h3&gt;Call and continue&lt;/h3&gt; &lt;p&gt;In the following example, I issue the &lt;code&gt;continue&lt;/code&gt; command to ask GDB to continue past that breakpoint seven times, stopping again the eighth time it is hit. By default, the &lt;code&gt;continue&lt;/code&gt; command causes the program to execute to the next breakpoint. Providing a numeric argument to this command tells GDB to continue that number of times without stopping at the intermediate breakpoints. After the &lt;code&gt;continue&lt;/code&gt; command stops, the &lt;code&gt;call&lt;/code&gt; command calls &lt;code&gt;print_tree()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) continue 8 Will ignore next 7 crossings of breakpoint 1.  Continuing. Breakpoint 1, insert (tree=0x4052a0, data=0x402055 "gecko") at tree.c:40 40      if (tree == NULL) (gdb) call print_tree(tree)   cat dog     javelina   wolf (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;GDB's printf command&lt;/h3&gt; &lt;p&gt;GDB also has a &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Output.html#index-printf"&gt;printf&lt;/a&gt; command, which I've used here:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) printf "tree is %lx and data is %s\n", tree, data tree is 4052a0 and data is gecko &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GDB's &lt;code&gt;printf&lt;/code&gt; command prints to GDB's console, not to the program output. For this example, we will probably find it more useful to call the program's &lt;code&gt;printf()&lt;/code&gt; function from the standard C library. It will print to the program's output, which is also where the program's &lt;code&gt;print_tree&lt;/code&gt; function prints its output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) call printf("tree is %lx and data is %s\n", tree, data) tree is 4052a0 and data is gecko $1 = 33 (gdb)  &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This output differs from GDB's built-in &lt;code&gt;printf&lt;/code&gt; command by printing an additional line (&lt;code&gt;$1 = 33&lt;/code&gt;). What's happening here is that GDB is calling &lt;code&gt;printf()&lt;/code&gt; to output the expected result. The &lt;code&gt;printf()&lt;/code&gt; function returns an integer representing the number of characters printed. This return value is printed to the GDB console and saved to the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Value-History.html#Value-History"&gt;value history&lt;/a&gt;. If you want to suppress the printing of the return value (as well as its appearance in the value history), cast the return value of &lt;code&gt;printf()&lt;/code&gt; to &lt;code&gt;void&lt;/code&gt;, like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) call (void) printf("tree is %lx and data is %s\n", tree, data) tree is 4052a0 and data is gecko &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Attaching commands to a breakpoint&lt;/h2&gt; &lt;p&gt;We are now ready to use GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Break-Commands.html#index-breakpoint-commands"&gt;commands&lt;/a&gt; command to attach a list of commands to a previously set breakpoint or list of breakpoints. When no breakpoint number or list is provided, &lt;code&gt;commands&lt;/code&gt; adds commands to the most recently defined breakpoint. Assuming that to be the case for the breakpoint set in the previous section, we can use the &lt;code&gt;commands&lt;/code&gt; command as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) commands Type commands for breakpoint(s) 1, one per line. End with a line saying just "end". &gt;silent &gt;call (void) printf("Entering insert(tree=%lx, data=%s)\n", tree, data) &gt;if (tree != 0)  &gt;call (void) printf("Tree is...\n")  &gt;call (void) print_tree(tree)  &gt;end &gt;continue &gt;end (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first command associated with the breakpoint is &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Break-Commands.html#index-silent"&gt;silent&lt;/a&gt;. It tells GDB not to print the usual messages that are printed when stopping at a breakpoint.&lt;/p&gt; &lt;p&gt;Next is a &lt;code&gt;call&lt;/code&gt; command. It invokes &lt;code&gt;printf()&lt;/code&gt;, which prints a message showing that the &lt;code&gt;insert()&lt;/code&gt; function has been entered along with the values of &lt;code&gt;tree&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Next comes an &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Command-Files.html#index-if"&gt;if&lt;/a&gt; command. It checks whether the value of &lt;code&gt;tree&lt;/code&gt; is non-zero (that is, non-&lt;code&gt;NULL&lt;/code&gt;). If this condition is true, then the two &lt;code&gt;call&lt;/code&gt; commands are executed, because there is data in &lt;code&gt;tree&lt;/code&gt;. If not, those &lt;code&gt;call&lt;/code&gt; commands are skipped. The &lt;code&gt;end&lt;/code&gt; command terminates the block of commands for the &lt;code&gt;if&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;A &lt;code&gt;continue&lt;/code&gt; command comes next. It causes GDB to resume execution until either another breakpoint is hit or the program terminates.&lt;/p&gt; &lt;p&gt;Finally, an &lt;code&gt;end&lt;/code&gt; command terminates the list of commands to attach to the breakpoint.&lt;/p&gt; &lt;p&gt;With the breakpoint and its associated commands in place, running the program produces the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) run The program being debugged has been started already. Start it from the beginning? (y or n) y Starting program: /home/kev/ctests/tree  Entering insert(tree=0, data=dog) Entering insert(tree=4056e0, data=cat) Tree is... dog Entering insert(tree=0, data=cat) Entering insert(tree=4056e0, data=wolf) Tree is...   cat dog ... Entering insert(tree=405970, data=scorpion) Tree is...   gecko javelina Entering insert(tree=0, data=scorpion) cat coyote dog gecko javelina scorpion wolf    cat     coyote dog       gecko     javelina       scorpion   wolf [Inferior 1 (process 326307) exited normally] &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Saving the insert function breakpoint&lt;/h2&gt; &lt;p&gt;Let's use the &lt;code&gt;info breakpoints&lt;/code&gt; command to look at the breakpoint for &lt;code&gt;insert&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) info breakpoints  Num     Type           Disp Enb Address            What 1       breakpoint     keep y   0x000000000040127a in insert at tree.c:40     breakpoint already hit 19 times         silent         call (void) printf("Entering insert(tree=%lx, data=%s)\n", tree, data)         if (tree != 0)           call (void) printf("Tree is...\n")           call (void) print_tree(tree)         end         continue (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Observe that the &lt;code&gt;info breakpoints&lt;/code&gt; output shows the number of times that the breakpoint has been hit. In this program, we see that &lt;code&gt;insert&lt;/code&gt; was called 19 times. Although it's not especially relevant for the current discussion, knowing how many times a particular function was called might be useful for optimization or performance analysis.&lt;/p&gt; &lt;p&gt;Let's save this breakpoint to a file named &lt;code&gt;my-insert-breakpoint&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) save breakpoints my-insert-breakpoint Saved to file 'my-insert-breakpoint'. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;my-insert-breakpoint&lt;/code&gt; file now contains GDB commands that, when run, will recreate the &lt;code&gt;insert()&lt;/code&gt; breakpoint plus its associated commands for use in a future GDB session:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;break tree.c:insert   commands     silent     call (void) printf("Entering insert(tree=%lx, data=%s)\n", tree, data)     if (tree != 0)       call (void) printf("Tree is...\n")       call (void) print_tree(tree)     end     continue   end &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Running a program with insert and dprintf breakpoints&lt;/h2&gt; &lt;p&gt;I now have two files with saved breakpoints, one named &lt;code&gt;my-dprintf-breakpoints&lt;/code&gt; and the other named &lt;code&gt;my-insert-breakpoint&lt;/code&gt;. Let's start GDB, load the &lt;code&gt;dprintf&lt;/code&gt; and &lt;code&gt;breakpoint&lt;/code&gt; commands listed in the files, and then run the program with output redirected to &lt;code&gt;my-program-output&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;$ gdb -q ./tree Reading symbols from ./tree... (gdb) source my-dprintf-breakpoints Dprintf 1 at 0x401281: file tree.c, line 41. Dprintf 2 at 0x4012b9: file tree.c, line 47. Dprintf 3 at 0x4012de: file tree.c, line 49. (gdb) source my-insert-breakpoint Breakpoint 4 at 0x40127a: file tree.c, line 40. (gdb) set dprintf-style call (gdb) run &gt;my-program-output Starting program: /home/kev/ctests/tree &gt;my-program-output [Inferior 1 (process 327130) exited normally] (gdb) quit &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the &lt;code&gt;set dprintf-style call&lt;/code&gt; command had not been automatically added to either of the files loaded via the &lt;code&gt;source&lt;/code&gt; command. It might make sense to manually add it to the &lt;code&gt;my-dprintf-breakpoints&lt;/code&gt; file. Alternately, it could be placed into another file—let's call it &lt;code&gt;tree-debugging-commands&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;file tree source my-dprintf-breakpoints source my-insert-breakpoint set dprintf-style call&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This file, &lt;code&gt;tree-debugging-commands&lt;/code&gt;, first specifies the program to debug via the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Files.html#index-file"&gt;file&lt;/a&gt; command. In earlier examples, we caused &lt;code&gt;tree&lt;/code&gt; to be loaded by mentioning it on the &lt;code&gt;gdb&lt;/code&gt; command line; here, however, we don't list it on the command line, but instead cause it to be loaded via the &lt;code&gt;file&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;The remaining commands in &lt;code&gt;tree-debugging-commands&lt;/code&gt; should be familiar by now. Commands contained in the &lt;code&gt;my-dprintf-breakpoints&lt;/code&gt; and &lt;code&gt;my-insert-breakpoints&lt;/code&gt; files are executed, followed by the &lt;code&gt;set dprintf-style call&lt;/code&gt; command. Recall that this command causes &lt;code&gt;dprintf&lt;/code&gt; breakpoints to call &lt;code&gt;printf()&lt;/code&gt; in the program being debugged (instead of using GDB's internal &lt;code&gt;printf&lt;/code&gt; command).&lt;/p&gt; &lt;p&gt;With that file in place, we can run GDB as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;$ gdb -q -x tree-debugging-commands Dprintf 1 at 0x401281: file tree.c, line 41. Dprintf 2 at 0x4012b9: file tree.c, line 47. Dprintf 3 at 0x4012de: file tree.c, line 49. Breakpoint 4 at 0x40127a: file tree.c, line 40. (gdb) run &gt;my-program-output Starting program: /home/kev/ctests/tree &gt;my-program-output [Inferior 1 (process 351102) exited normally] (gdb) quit&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Additional commands&lt;/h3&gt; &lt;p&gt;It &lt;em&gt;should&lt;/em&gt; also be possible to achieve the same effect, but without needing to interact with GDB, by using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gdb -q -x tree-debugging-commands -ex 'run &gt;my-program-output' -ex quit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As noted in &lt;a href="https://developers.redhat.com/articles/2021/10/05/printf-style-debugging-using-gdb-part-1"&gt;Part 1&lt;/a&gt; of this series, the &lt;code&gt;-q&lt;/code&gt; option suppresses the GDB banner, copyright, and help information when GDB starts up. The &lt;code&gt;-x &lt;/code&gt;option, in this case, causes GDB to load and execute the commands from the file &lt;code&gt;tree-debugging-commands&lt;/code&gt;. The &lt;code&gt;-ex&lt;/code&gt; options cause the command following the option to be run. So, in this case, after loading and running the commands in &lt;code&gt;tree-debugging-commands&lt;/code&gt;, a &lt;code&gt;run&lt;/code&gt; command is issued from the first &lt;code&gt;-ex&lt;/code&gt; option; moreover, the output from the run is redirected to the file &lt;code&gt;my-program-output&lt;/code&gt;. The command following the second &lt;code&gt;-ex&lt;/code&gt; option is &lt;code&gt;quit&lt;/code&gt;; this causes GDB to quit without ever showing a prompt.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;run&lt;/code&gt; and &lt;code&gt;quit&lt;/code&gt; commands could also be placed in the command file, &lt;code&gt;tree-debugging-commands&lt;/code&gt;. If this were done, the command line would be shortened to look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gdb -q -x tree-debugging-commands&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;A bugfix for dprintf breakpoints&lt;/h3&gt; &lt;p&gt;While writing this article, I discovered &lt;a href="https://sourceware.org/bugzilla/show_bug.cgi?id=28308"&gt;a bug in GDB&lt;/a&gt; that caused &lt;code&gt;dprintf&lt;/code&gt; breakpoints to be (essentially) disabled when running a program from the command line or from within a GDB script. This bug has been fixed in the upstream GDB sources. On Fedora Linux, &lt;code&gt;gdb-11.1-5&lt;/code&gt; (and later) contain this fix. If you are using a version of GDB without this fix, you will need to issue the &lt;code&gt;run&lt;/code&gt; command from the GDB prompt.&lt;/p&gt; &lt;h2&gt;Go further with GDB&lt;/h2&gt; &lt;p&gt;I hope this series has been useful to developers familiar with debugging their code using print statements, but who previously had little or no familiarity with GDB. I also hope that it whets your appetite for doing more with GDB.&lt;/p&gt; &lt;p&gt;This article demonstrated how to set a breakpoint and run until it is hit, a very common use of GDB. Once GDB is stopped at a breakpoint, you can enter a variety of GDB commands to reveal more about the state of the program at that point. If you want to go further with GDB commands, I recommend the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger"&gt;The GDB developer's GNU Debugger tutorial, Part 1&lt;/a&gt; is a general guide to debugging with the GNU Debugger.&lt;/li&gt; &lt;li&gt;&lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/"&gt;Debugging with GDB&lt;/a&gt;, the GDB manual, is the authoritative reference for using GDB.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/09/printf-style-debugging-using-gdb-part-3" title="Printf-style debugging using GDB, Part 3"&gt;Printf-style debugging using GDB, Part 3&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Buettner</dc:creator><dc:date>2021-12-09T07:00:00Z</dc:date></entry><entry><title>Node.js serverless functions on Red Hat OpenShift, Part 3: Debugging on a cluster</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/08/nodejs-serverless-functions-red-hat-openshift-part-3-debugging-cluster" /><author><name>Lucas Holmquist</name></author><id>5f09a83d-1c8f-4320-9046-cd564135b8d1</id><updated>2021-12-08T07:00:00Z</updated><published>2021-12-08T07:00:00Z</published><summary type="html">&lt;p&gt;This article is the third in a series about running &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; applications in &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; in an orchestration environment such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; or &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The &lt;a href="https://developers.redhat.com/articles/2021/07/01/nodejs-serverless-functions-red-hat-openshift-part-1-logging"&gt;first article&lt;/a&gt; focused on control over logging, and the &lt;a href="https://developers.redhat.com/articles/2021/07/13/nodejs-serverless-functions-red-hat-openshift-part-2-debugging-locally"&gt;second article&lt;/a&gt; on debugging &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; programs on your local system. This time, we'll look at how to use the &lt;a href="https://developer.chrome.com/docs/devtools/"&gt;Chrome DevTools&lt;/a&gt; inspector to debug a Node.js function that is running inside a container on an OpenShift cluster.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow along, you will need an OpenShift cluster with the Serverless Operator installed. See the article, &lt;a href="blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions"&gt;Create your first serverless function with Red Hat OpenShift Serverless Functions&lt;/a&gt; to set up this environment.&lt;/p&gt; &lt;p&gt;This article also assumes some familiarity with the Chrome DevTools inspector. For an introduction, please see &lt;a href="https://developers.redhat.com/blog/2018/05/15/debug-your-node-js-application-on-openshift-with-chrome-devtools"&gt;How to debug your Node.js application on OpenShift with Chrome DevTools&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Get the example code&lt;/h2&gt; &lt;p&gt;The example that we are going to use can be retrieved from the &lt;a href="https://github.com/nodeshift-blog-examples/debugging-with-functions-on-cluster"&gt;Node.js HTTP function repository&lt;/a&gt; on GitHub. Similar to the example used in our &lt;a href="https://developers.redhat.com/articles/2021/07/13/nodejs-serverless-functions-red-hat-openshift-part-2-debugging-locally"&gt;previous article&lt;/a&gt;, this function application was scaffolded with the &lt;code&gt;kn-func&lt;/code&gt; command-line tool, which we described in that article.&lt;/p&gt; &lt;p&gt;We'll have to make a few changes before using the code for this article. To start, note that the &lt;code&gt;package.json&lt;/code&gt; file for this example defines three scripts:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;"scripts": { "test": "node test/unit.js &amp;&amp; node test/integration.js", "start": "faas-js-runtime ./index.js", "debug": "nodemon --inspect ./node_modules/faas-js-runtime/bin/cli.js ./index.js" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These scripts are similar to the ones in the previous article, and we will make the same changes here that we did previously.&lt;/p&gt; &lt;p&gt;First, simply switch the &lt;code&gt;debug&lt;/code&gt; script with the &lt;code&gt;start&lt;/code&gt; script. We make this change because the &lt;code&gt;kn-func&lt;/code&gt; command can’t specify which script to run. You can name the start script whatever you like.&lt;/p&gt; &lt;p&gt;Next, make a simple change to the start script. You need to tell it to listen on all available addresses because you are running as &lt;code&gt;localhost&lt;/code&gt; (IP address 127.0.0.1) inside the container, which the debugger can’t access by default. Therefore, change the &lt;code&gt;--inspect&lt;/code&gt; option in the start script to &lt;code&gt;--inspect=0.0.0.0&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;These changes should produce scripts similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;"scripts": { "test": "node test/unit.js &amp;&amp; node test/integration.js", "debug": "faas-js-runtime ./index.js", "start": "nodemon --inspect=0.0.0.0 ./node_modules/faas-js-runtime/bin/cli.js ./index.js" }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Debugging Node.js functions in a container&lt;/h2&gt; &lt;p&gt;Now, create a container using the &lt;code&gt;kn-func build&lt;/code&gt; command. If this is the first time you are building the application, the command prompts you to add a registry and namespace for the containerized function. By default, the registry is Docker Hub. For the namespace, enter your Docker Hub ID.&lt;/p&gt; &lt;p&gt;Once the image is built, use the &lt;code&gt;docker&lt;/code&gt; command to run the container and start debugging your Node.js functions. Since the debugger listens on port 9229, you need to expose that port as well as port 8080, which is the port to access your application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run --rm -p 8080:8080 -p 9229:9229 lholmquist/debugging-with-functions-on-cluster:latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output should be similar to:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Debugger listening on ws://0.0.0.0:9229/584eb679-4db1-4a40-9519-5bf5c42275f5 For help, see: https://nodejs.org/en/docs/inspector The server has started. http://localhost:8080&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that the container is running, you can open Chrome DevTools and navigate to the &lt;code&gt;chrome://inspect&lt;/code&gt; URL, where you should see a link labeled &lt;strong&gt;inspect&lt;/strong&gt; (Figure 1). Click this link to connect to your running container.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/inspect.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/inspect.png?itok=LAbz7U60" width="1019" height="474" alt="The Chrome inspector offers an inspection link to let you view and debug your program." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Use the Chrome inspector to view and debug your program. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now you should see the Chrome inspector, showing code similar to Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/breakpoint.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/breakpoint.png?itok=kk1rbv2J" width="1440" height="841" alt="The Chrome inspector shows the code of your application and allows you to set breakpoints." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Use the Chrome inspector to set breakpoints. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Set a breakpoint at some point in the program, then navigate to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; to run the application. The inspector stops at the breakpoint shown in Figure 3, allowing you to inspect variables and do other debugging tasks.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/stopped.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/stopped.png?itok=Tb4mGgUU" width="1280" height="752" alt="The program shows the breakpoint where it stopped." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. The program shows the breakpoint where it stopped. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Debugging Node.js functions on an OpenShift cluster&lt;/h2&gt; &lt;p&gt;Having debugged your program in your container, you can use a similar process to debug it on an OpenShift cluster. Make the same changes in the npm scripts and use the same command to build the container. To deploy the container to the cluster, use the &lt;code&gt;kn-func&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kn-func deploy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the container is deployed, navigate to the topology view in the OpenShift console, which should show you something like Figure 4.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/icon.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/icon.png?itok=1z_R1mXD" width="1440" height="697" alt="The topology view of a function deployed in OpenShift shows an icon you can click to run the function." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Deploy the function from the OpenShift topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Clicking the icon on the top-right corner of the function's box navigates to the application's route. You should then see the JSON output that the function sends when invoked.&lt;/p&gt; &lt;p&gt;To start debugging, you need the help of the &lt;code&gt;oc port-forward&lt;/code&gt; command. This command was described in the article &lt;a href="https://developers.redhat.com/blog/2018/05/15/debug-your-node-js-application-on-openshift-with-chrome-devtools"&gt;How to debug your Node.js application on OpenShift with Chrome DevTools&lt;/a&gt;, so I won't explain here what each part of the command does. For this example, your command should look something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc port-forward $(oc get po | grep debugging-with-functions | grep Running | awk '{print $1}') 8888:9229&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command should start forwarding requests to the debugger process. If it is successful, you'll see log messages similar to these:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Forwarding from 127.0.0.1:8888 -&gt; 9229 Forwarding from [::1]:8888 -&gt; 9229 Handling connection for 8888 Handling connection for 8888&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With forwarding enabled, navigate again in your browser to the chrome://inspect URL and you should see something similar to Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/debug.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/debug.png?itok=JAgfhBg9" width="1440" height="510" alt="The "inspect" link in Chrome inspector also allows you to debug on a cluster." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. The "inspect" link in Chrome inspector also allows you to debug on a cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;As in the previous example with the container, clicking the &lt;strong&gt;inspect&lt;/strong&gt; link should show the debugger. Again add a breakpoint, then navigate to the route that OpenShift provides to access the application. The debugger should break on the point you added, as shown in Figure 6.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cluster_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/cluster_0.png?itok=t4Ls2kWv" width="1280" height="747" alt="The program stops at the breakpoint in the cluster." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6. The program stops at the breakpoint in the cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article showed how to debug a Node.js application while running inside a container, as well as running on an OpenShift cluster.&lt;/p&gt; &lt;p&gt;Stay tuned for more posts related to running Node.js applications on &lt;a href="https://cloud.redhat.com/learn/topics/serverless"&gt;Red Hat OpenShift Serverless&lt;/a&gt;. You can also check out the latest documentation at the &lt;a href="https://openshift-knative.github.io/docs/docs/functions/about-functions.html"&gt;About OpenShift Serverless Functions&lt;/a&gt; site.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/08/nodejs-serverless-functions-red-hat-openshift-part-3-debugging-cluster" title="Node.js serverless functions on Red Hat OpenShift, Part 3: Debugging on a cluster"&gt;Node.js serverless functions on Red Hat OpenShift, Part 3: Debugging on a cluster&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-12-08T07:00:00Z</dc:date></entry><entry><title>Kafka Monthly Digest: November 2021</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/07/kafka-monthly-digest-november-2021" /><author><name>Mickael Maison</name></author><id>f2ddbf8a-384a-4c7c-9d61-45a079471984</id><updated>2021-12-07T20:00:00Z</updated><published>2021-12-07T20:00:00Z</published><summary type="html">&lt;p&gt;This 46th edition of the Kafka Monthly Digest covers what happened in the &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; community in November 2021, including the imminent release of Apache Kafka 3.1.0, notable Kafka Improvement Proposals (KIPs), community project releases for Jikkou 0.7 and AKHQ 0.19.0, and more.&lt;/p&gt; &lt;p&gt;For last month's digest, see &lt;a href="https://developers.redhat.com/articles/2021/11/10/kafka-monthly-digest-october-2021"&gt;Kafka Monthly Digest: October 2021&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Releases&lt;/h2&gt; &lt;p&gt;Two new Kafka bugfix versions were released in November, and Kafka 3.1.0 is in progress.&lt;/p&gt; &lt;h3&gt;Bugfix releases for Kafka 2.7.2 and 2.6.3&lt;/h3&gt; &lt;p&gt;I &lt;a href="https://twitter.com/apachekafka/status/1462848436156256259"&gt;released&lt;/a&gt; Kafka 2.6.3 and 2.7.2 on November 22. These two bugfix releases include a fix for &lt;a href="https://kafka.apache.org/cve-list"&gt;CVE-2021-38153&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;p&gt;2.6.3 addresses &lt;a href="https://issues.apache.org/jira/secure/IssueNavigator.jspa?reset=true&amp;jqlQuery=project+%3D+KAFKA+AND+fixVersion+%3D+2.6.3+AND+resolution+%21%3D+Unresolved+ORDER+BY+priority+DESC%2C+status+DESC%2C+updated+DESC++++++++++++++++++++++&amp;src=confmacro"&gt;11 JIRA tickets&lt;/a&gt;. For more details, see the &lt;a href="https://www.apache.org/dist/kafka/2.6.3/RELEASE_NOTES.html"&gt;release notes&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;2.7.2 addresses &lt;a href="https://issues.apache.org/jira/secure/IssueNavigator.jspa?reset=true&amp;jqlQuery=project+%3D+KAFKA+AND+fixVersion+%3D+2.7.2+AND+resolution+%21%3D+Unresolved+ORDER+BY+priority+DESC%2C+status+DESC%2C+updated+DESC+++++++++++++++++++++&amp;src=confmacro"&gt;26 JIRA tickets&lt;/a&gt;. For more details, see the &lt;a href="https://downloads.apache.org/kafka/2.7.2/RELEASE_NOTES.html"&gt;release notes&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Kafka 3.1.0 is nearly here&lt;/h3&gt; &lt;p&gt;The next minor Kafka version, 3.1.0, is also nearing release. Code freeze happened on November 12 and we are now in the stabilization phase. There are still a &lt;a href="https://issues.apache.org/jira/browse/KAFKA-13469?jql=project%20%3D%20KAFKA%20AND%20fixVersion%20%3D%203.1.0%20AND%20status%20not%20in%20(resolved%2C%20closed)%20ORDER%20BY%20priority%20DESC%2C%20status%20DESC%2C%20updated%20DESC%20%20%20%20%20%20"&gt;few open blockers&lt;/a&gt; but it should be ready in the next couple of weeks.&lt;/p&gt; &lt;h2&gt;Kafka Improvement Proposals&lt;/h2&gt; &lt;p&gt;Last month, the community submitted 16 &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals"&gt;KIPs&lt;/a&gt; (KIP-789 to KIP-805; note that 790 was skipped). I'll highlight just a few of them:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-796%3A+Interactive+Query+v2"&gt;&lt;strong&gt;KIP-796: Interactive Query v2&lt;/strong&gt;&lt;/a&gt;: Interactive Query is a functionality in Kafka Streams that allows you to query the internal state of Streams processors. The API dates back to the early days of Streams and has a few problems and limitations. This KIP proposes a brand new API, which is designed to be easier to use and maintain and will also offer more flexibility for users.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-800%3A+Add+reason+to+JoinGroupRequest+and+LeaveGroupRequest"&gt;&lt;strong&gt;KIP-800: Add reason to JoinGroupRequest and LeaveGroupRequest&lt;/strong&gt;&lt;/a&gt;: It can be hard to troubleshoot rebalancing issues because relevant logs are spread between brokers and clients. This KIP would make it simpler to troubleshoot such issues by requiring that consumers provide a reason when they join or leave a group.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-801%3A+Implement+an+Authorizer+that+stores+metadata+in+__cluster_metadata"&gt;&lt;strong&gt;KIP-801: Implement an Authorizer that stores metadata in __cluster_metadata&lt;/strong&gt;&lt;/a&gt;: By default, Kafka has a built-in authorizer implementation, &lt;code&gt;AclAuthorizer&lt;/code&gt;, that stores access control lists (ACLs) in ZooKeeper. As Kafka is moving away from ZooKeeper (see &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum"&gt;KIP-500&lt;/a&gt; for more details), this KIP proposes replacing the built-in authorizer with a new fully compatible implementation that stores ACLs directly in Kafka.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-802%3A+Validation+Support+for+Kafka+Connect+SMT+Options"&gt;&lt;strong&gt;KIP-802: Validation support for Kafka Connect SMT options&lt;/strong&gt;&lt;/a&gt;: Kafka Connect has a validation endpoint that you can use to check a connector's configuration before starting it. However, this endpoint does not currently validate &lt;a href="https://kafka.apache.org/documentation/#connect_transforms"&gt;Single Message Transformations&lt;/a&gt; (SMT), so the only way to know if an SMT configuration is correct is to try starting a connector. This KIP aims to address this gap by updating the &lt;code&gt;validate&lt;/code&gt; endpoint to check the full configuration.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Community releases&lt;/h2&gt; &lt;p&gt;This section covers a few notable &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; community project releases:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/streamthoughts/jikkou/releases/tag/v0.7.0"&gt;&lt;strong&gt;Jikkou 0.7&lt;/strong&gt;&lt;/a&gt;: Jikkou is a command-line tool for automating Kafka topic management. The latest version supports adding partitions to topics, and the topic validation options have been improved.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/tchiotludo/akhq/releases/tag/0.19.0"&gt;&lt;strong&gt;AKHQ 0.19.0&lt;/strong&gt;&lt;/a&gt;: AKHQ is a GUI for Apache Kafka. Version 0.19 adds support for JSON and Protobuf schemas. There are also various improvements to the authentication options and to the topic data explorer, which now handles the &lt;code&gt;__consumer_offset&lt;/code&gt; and &lt;code&gt;__transaction_state&lt;/code&gt; internal topics. A number of fixes are included in this version, as well.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/Blizzard/node-rdkafka"&gt;&lt;strong&gt;node-rdkafka 2.12&lt;/strong&gt;&lt;/a&gt;: This new release of node-rdkafka is now based on &lt;a href="https://github.com/edenhill/librdkafka"&gt;librdkafka&lt;/a&gt; 1.7.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Blogs&lt;/h2&gt; &lt;p&gt;I recommend the following blogs and articles that were published last month:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://medium.com/bigpanda-engineering/sleeping-good-at-night-kafka-configurations-tweaks-6dd4d3aaf4e5"&gt;Sleeping good at night: Kafka configurations tweaks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://debezium.io/blog/2021/11/23/debezium-ui-transforms/"&gt;Debezium UI support for Single Message Transformations&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://medium.com/paypal-tech/kafka-consumer-benchmarking-c726fbe4000"&gt;Scaling Kafka Consumer for billions of events&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://piotrminkowski.com/2021/11/24/kafka-streams-with-quarkus/"&gt;Kafka Streams with Quarkus&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To learn more about Kafka, visit Red Hat Developer's &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka topic page&lt;/a&gt; or the &lt;a href="https://kafka.apache.org"&gt;Kafka product homepage&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/07/kafka-monthly-digest-november-2021" title="Kafka Monthly Digest: November 2021"&gt;Kafka Monthly Digest: November 2021&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mickael Maison</dc:creator><dc:date>2021-12-07T20:00:00Z</dc:date></entry><entry><title>Visualize your Apache Kafka Streams using the Quarkus Dev UI</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/07/visualize-your-apache-kafka-streams-using-quarkus-dev-ui" /><author><name>Daniel Oh</name></author><id>2667143c-ad01-4773-80dd-4d708894a815</id><updated>2021-12-07T07:00:00Z</updated><published>2021-12-07T07:00:00Z</published><summary type="html">&lt;p&gt;This article shows how you can visualize &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Apache Kafka Streams&lt;/a&gt; with reactive applications using the Dev UI in &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;. Quarkus, a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; framework, provides an extension to utilize the Kafka Streams API and also lets you implement stream processing applications based directly on &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reactive messaging and Apache Kafka&lt;/h2&gt; &lt;p&gt;With the rise of &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven architectures&lt;/a&gt;, many developers are adopting reactive programming to write business applications. The requirements for these applications literally specify that they not be processed in real-time because end users don't really expect synchronous communication experiences through web browsers or mobile devices. Instead, low latency is a more important performance criterion, regardless of data volume or concurrent users.&lt;/p&gt; &lt;p&gt;You might be wondering how reactive programming could meet this very different goal. The secret is an asynchronous communication protocol that decouples senders from the applications that consume and process events. In this design, a caller (e.g., end user) sends a message to a recipient and then keeps processing other requests without waiting for the reply. Asynchronous processing can also improve high-volume data performance, security, and scalability.&lt;/p&gt; &lt;p&gt;However, it's not easy to implement everything involved in asynchronous communication capabilities with just reactive programming. This is the reason that message-queue platforms have also come to occupy a critical role in event-driven applications. &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; is one of the most popular platforms for processing event messages asynchronously to support reactive applications. &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams&lt;/a&gt; is a client library that abstracts changing event data sets (also known as &lt;em&gt;streams&lt;/em&gt;) continuously in Kafka clusters to support high throughput and scalability. A stream is a collection of data records in the form of key-value pairs.&lt;/p&gt; &lt;h2&gt;Example: Using the Quarkus Dev UI&lt;/h2&gt; &lt;p&gt;Take a look at the following &lt;code&gt;getMetaData()&lt;/code&gt; method to see how Quarkus lets you issue interactive queries to Kafka Streams using a &lt;code&gt;KafkaStreams&lt;/code&gt; injection. Find the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/blob/main/kafka-streams-quickstart/aggregator/src/main/java/org/acme/kafka/streams/aggregator/streams/InteractiveQueries.java"&gt;complete code&lt;/a&gt; in the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/blob/main/kafka-streams-quickstart"&gt;Quarkus Kafka Streams Quickstart&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; @Inject KafkaStreams streams; public List&lt;PipelineMetadata&gt; getMetaData() { return streams.allMetadataForStore(TopologyProducer.WEATHER_STATIONS_STORE) .stream() .map(m -&gt; new PipelineMetadata( m.hostInfo().host() + ":" + m.hostInfo().port(), m.topicPartitions() .stream() .map(TopicPartition::toString) .collect(Collectors.toSet()))) .collect(Collectors.toList()); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Kafka Streams also lets you build a process topology that represents a graph of sources, processors, and sinks in Kafka topics. Of course, you can monitor the streams using command-line tools (such as &lt;a href="https://docs.confluent.io/platform/current/app-development/kafkacat-usage.html"&gt;kcat&lt;/a&gt;), but the text-based output doesn't make it easy to understand how the streams are processing and consuming messages across Kafka topics.&lt;/p&gt; &lt;p&gt;Take a look at another example. The &lt;code&gt;buildTopology()&lt;/code&gt; method lets you build the stream's topology. Find the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/blob/main/kafka-streams-quickstart/aggregator/src/main/java/org/acme/kafka/streams/aggregator/streams/TopologyProducer.java"&gt;complete code&lt;/a&gt; in the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/blob/main/kafka-streams-quickstart"&gt;Quarkus Kafka Streams Quickstart&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; @Produces public Topology buildTopology() { StreamsBuilder builder = new StreamsBuilder(); ObjectMapperSerde&lt;WeatherStation&gt; weatherStationSerde = new ObjectMapperSerde&lt;&gt;(WeatherStation.class); ObjectMapperSerde&lt;Aggregation&gt; aggregationSerde = new ObjectMapperSerde&lt;&gt;(Aggregation.class); KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(WEATHER_STATIONS_STORE); GlobalKTable&lt;Integer, WeatherStation&gt; stations = builder.globalTable( WEATHER_STATIONS_TOPIC, Consumed.with(Serdes.Integer(), weatherStationSerde)); builder.stream( TEMPERATURE_VALUES_TOPIC, Consumed.with(Serdes.Integer(), Serdes.String())) .join( stations, (stationId, timestampAndValue) -&gt; stationId, (timestampAndValue, station) -&gt; { String[] parts = timestampAndValue.split(";"); return new TemperatureMeasurement(station.id, station.name, Instant.parse(parts[0]), Double.valueOf(parts[1])); }) .groupByKey() .aggregate( Aggregation::new, (stationId, value, aggregation) -&gt; aggregation.updateFrom(value), Materialized.&lt;Integer, Aggregation&gt; as(storeSupplier) .withKeySerde(Serdes.Integer()) .withValueSerde(aggregationSerde)) .toStream() .to( TEMPERATURES_AGGREGATED_TOPIC, Produced.with(Serdes.Integer(), aggregationSerde)); return builder.build(); } &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Visualize the Kafka Streams topology&lt;/h3&gt; &lt;p&gt;To visualize the Kafka Streams topology, developers traditionally needed additional visualizer tools that run in the cloud or local development environments separately from Kafka clusters. But Quarkus's built-in Dev UI lets you see all the extensions currently loaded with relevant documentation. When you run Quarkus Dev Mode (e.g., &lt;code&gt;./mvnw quarkus:dev&lt;/code&gt;) and add a &lt;code&gt;quarkus-kafka-streams&lt;/code&gt; extension in a project, the Dev UI shows the Apache Kafka Streams extension graphically (Figure 1).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dev.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/dev.png?itok=RF3UnaLx" width="1440" height="782" alt="The Developer UI shows the Apache Kafka Streams extension, with a Topology button." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Developer UI shows the Apache Kafka Streams extension, with a Topology button. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;When you click on the &lt;strong&gt;Topology&lt;/strong&gt; icon, it brings you to the Kafka Streams topology UI (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topic_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/topic_0.png?itok=BU-xIRU6" width="1440" height="966" alt="The Topology screen for Apache Kafka Streams shows details, including active topics." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The Topology screen for Apache Kafka Streams shows details, including active topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The topology UI shows how the event streams sink in topics (e.g., &lt;code&gt;temperature-values&lt;/code&gt;) and how Quarkus applications consume the streams from the topics. Also, you can understand how the application eventually aggregates streams from multiple topics (&lt;code&gt;temperature-values&lt;/code&gt; and &lt;code&gt;weather-stations&lt;/code&gt;) to one topic (&lt;code&gt;temperatures-aggregated&lt;/code&gt;). The Topology UI also showcases the sequences on how the streams are sourced, joined, and aggregated continuously in Kafka clusters.&lt;/p&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;This article has shown how to visualize Kafka Streams with Quarkus applications and the Dev UI. Quarkus also provides awesome features to improve your productivity through &lt;a href="https://quarkus.io/guides/continuous-testing"&gt;continuous testing&lt;/a&gt;, the &lt;a href="https://quarkus.io/guides/cli-tooling"&gt;Quarkus command-line interface (CLI)&lt;/a&gt;, and &lt;a href="https://quarkus.io/guides/dev-services"&gt;Dev Services&lt;/a&gt;. To learn more about Kafka and reactive messaging programming, see the following articles:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://quarkus.io/guides/kafka-reactive-getting-started"&gt;Getting Started to SmallRye Reactive Messaging with Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;How do I run Apache Kafka on Kubernetes?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/devnation/tech-talks/gaming-telemetry-kafka"&gt;Level-up your gaming telemetry using Kafka Streams&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/devnation/tech-talks/dual-writes-kafka-debezium"&gt;Outbox pattern with OpenShift Streams for Apache Kafka and Debezium&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/devnation/tech-talks/kafka-at-edge"&gt;Kafka at the Edge: an IoT scenario with OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/07/visualize-your-apache-kafka-streams-using-quarkus-dev-ui" title="Visualize your Apache Kafka Streams using the Quarkus Dev UI"&gt;Visualize your Apache Kafka Streams using the Quarkus Dev UI&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Oh</dc:creator><dc:date>2021-12-07T07:00:00Z</dc:date></entry><entry><title type="html">OptaPlanner documentation turns over a new leaf</title><link rel="alternate" href="https://blog.kie.org/2021/12/optaplanner-documentation-turns-over-a-new-leaf.html" /><author><name>rsynek</name></author><id>https://blog.kie.org/2021/12/optaplanner-documentation-turns-over-a-new-leaf.html</id><updated>2021-12-07T00:00:00Z</updated><content type="html">  For years, has been offering the documentation in two formats: single-page HTML and PDF. This now changes with the launch a new documentation website, built using . What’s so exciting about the new documentation? First and foremost, it loads instantly as opposed to the old single-page HTML documentation. For example, if I want to read about repeated planning, I open the single-page HTML docs and wait nearly half a minute for the page to load, despite having a very good cable connection. With the new documentation, it took me only 2 seconds as each chapter has its own HTML page and thus the content that has to be loaded is limited. This also means it’s now easier to effectively share links to a particular section. Second, now you can search through the entire docs: The search box at the top of the page shows suggestions as soon as you start typing. Each suggestion consists of the chapter and a link to where the search term occurs. Third, if you spot a discrepancy in the documentation and would like to improve it, contributing was never easier: Finally, this new documentation website is much friendlier to search engines, which should make it show up in Google search results more often than before. BUILDING THE DOCUMENTATION WEBSITE The documentation sources remain in the , but the website assembly, named optaplanner-website-docs, became a part of the : Similarly to the entire optaplanner-website, the optaplanner-website-docs is built using Maven. The Maven module acts as a wrapper over Antora, which generates the static site from AsciiDoc sources. There are two Antora playbooks referring to documentation sources. The first one, used by default, is antora-playbook.yml that refers to the latest OptaPlanner release. ... content: edit_url: '{web_url}/edit/main/{path}' sources: - url: git@github.com:kiegroup/optaplanner.git # Updates with every release to point to the latest release branch. branches: [8.12.x] start_path: optaplanner-docs/src ... To render the latest documentation: 1. cd optaplanner-website/optaplanner-website-docs 2. mvn clean package 3. Open the index.html located in target/website/docs in your browser. The second Antora playbook, antora-playbook-author.yml, is activated by the author maven profile and refers to the current optaplanner local Git repository HEAD. ... content: edit_url: '{web_url}/edit/main/{path}' sources: # Assuming the optaplanner local repository resides next to the optaplanner-website. - url: ../../optaplanner branches: [HEAD] start_path: optaplanner-docs/src ... To preview local changes in the documentation: 1. Make sure the optaplanner and optaplanner-website Git repositories are located in the same directory or change the local URL accordingly. 2. cd optaplanner-website/optaplanner-website-docs 3. mvn clean package -Pauthor 4. Open the index.html located in target/website/docs in your browser. SUPPORTED DOCUMENTATION FORMATS Introducing new formats does not have to result in abandoning the old ones, and in our case it does not. While I encourage everyone to visit the new documentation website, the existing formats continue to be published with every release as before. Also, should you need to have a look at a particular version of the documentation, it’s still at your hand in the . CONCLUSION Since the 8.12.0.Final release, there is a new documentation website available under . The documentation is now structured into pages by chapters and searchable. The single-page HTML and PDF documentation remains available for every release. ") The post appeared first on .</content><dc:creator>rsynek</dc:creator></entry><entry><title>Custom WebAssembly extensions in OpenShift Service Mesh</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/06/custom-webassembly-extensions-openshift-service-mesh" /><author><name>Satya Jayanti</name></author><id>fc6fbe22-868d-4c88-8402-f7fe8fef3b25</id><updated>2021-12-06T07:00:00Z</updated><published>2021-12-06T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html-single/service_mesh/index"&gt;Red Hat OpenShift Service Mesh 2.1&lt;/a&gt; requires using &lt;a href="https://webassembly.org"&gt;WebAssembly&lt;/a&gt; extensions instead of Istio Mixer to extend Service Mesh functionality. The 2.11 release of &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.11"&gt;Red Hat 3scale API Management&lt;/a&gt; also supports using WebAssembly extensions. Thus, the latest release of the 3scale integration for Istio uses the WebAssembly proxy instead of the Istio Mixer component.&lt;/p&gt; &lt;p&gt;Developers can use WebAssembly extensions in OpenShift Service Mesh and 3scale to add features directly to the &lt;a href="https://www.envoyproxy.io/"&gt;Envoy proxy&lt;/a&gt;, thereby moving common functionality out of applications and into the sidecar. Examples of useful extensions include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Manipulating headers for the request or response.&lt;/li&gt; &lt;li&gt;Adding authorization or custom rate limiting to requests.&lt;/li&gt; &lt;li&gt;Adding an additional data store or cache for side-channel request processing.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This article introduces the new WebAssembly proxy for OpenShift Service Mesh, with an example demonstrating how to configure an extension using 3scale.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Red Hat developers working on this integration are speaking at &lt;a href="https://www.apidays.global/paris/"&gt;Apidays LIVE Paris - APIs and the Future of Software&lt;/a&gt;, a virtual event happening December 7 through 9, 2021. See the end of the article for details and a video demonstration showing how to configure the new 3scale WebAssembly extension in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What is WebAssembly?&lt;/h2&gt; &lt;p&gt;WebAssembly (sometimes called Wasm) is a binary instruction format for stack-based virtual machines (VMs). The code is executed at near-native speed in a memory-safe sandbox, with clearly defined resource constraints and an API for communicating with the embedding host environment. Although the standards were developed to work for the web and to be executed in web browsers, WebAssembly can be embedded in non-web applications as well.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/proxy-wasm/spec"&gt;Proxy-Wasm&lt;/a&gt; specification provides conventions to use between hosts and their extensions delivered as WebAssembly modules. Hosts are typically transport-layer (L4) or application-layer (L7) proxies but can be other host environments, as well. Although the specification was primarily developed for using &lt;a href="https://github.com/proxy-wasm/spec/blob/master/docs/WebAssembly-in-Envoy.md"&gt;WebAssembly in Envoy&lt;/a&gt;, it is proxy-agnostic and can be used with different proxies.&lt;/p&gt; &lt;p&gt;The specification defines the set of interfaces, commonly called application binary interfaces (ABIs), in order to facilitate WebAssembly extension development. Proxy-Wasm extensions can be written in any language targeting WebAssembly and adhering to the interface library in the languages provided by Envoy: C/C++ using Emscripten, Rust, Go, and TypeScript. The extensions are compiled to bytecode and run inside a virtual machine sandbox with a well-defined interface to external applications using an HTTP proxy.&lt;/p&gt; &lt;h2&gt;WebAssembly in Istio and Service Mesh&lt;/h2&gt; &lt;p&gt;WebAssembly has been included with &lt;a href="https://istio.io/"&gt;Istio&lt;/a&gt;, the &lt;a href="https://developers.redhat.com/topics/service-mesh"&gt;OpenShift Service Mesh&lt;/a&gt; upstream project, since version 1.5, and was introduced to OpenShift Service Mesh 2.0 as a tech preview. The Service Mesh 2.1 release includes Istio 1.9, which standardizes on using extensions from &lt;a href="https://github.com/proxy-wasm/spec"&gt;WebAssembly for Proxies&lt;/a&gt; (Proxy-Wasm). Proxy-Wasm uses HTTP filters provided by the &lt;a href="https://www.envoyproxy.io/"&gt;Envoy proxy&lt;/a&gt; to extend common functionality.&lt;/p&gt; &lt;h3&gt;Advantages of WebAssembly over Istio Mixer&lt;/h3&gt; &lt;p&gt;Previously, Istio used a mixer component to handle the collection of telemetry data across the many Envoy proxies that make up a data plane (Figure 1). This central component also made it possible to add extensions for creating custom policies and metrics that apply across the mesh.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/proxy-wasm-fig1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/proxy-wasm-fig1.png?itok=rxoaghnp" width="600" height="357" alt="A diagram of the Istio mixer component implementing the mixer adapter pattern." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Istio Mixer component. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;This design had multiple drawbacks. Most notably, the central point could become a bottleneck and a resource hog when there was a large amount of traffic. Having a central point also creates a single point of failure.&lt;/p&gt; &lt;p&gt;Mixer was deprecated in OpenShift Service Mesh 2.0, and has now been completely removed. You can't even upgrade to OpenShift Service Mesh 2.1 while the Istio Mixer component is active. Instead, OpenShift Service Mesh embraced WebAssembly extensions for extending Envoy. The new Telemetry v2 architecture, introduced previously, replaces the mixer component's telemetry functions. The Istio and Envoy developer communities have been building WebAssembly extensibility into Envoy since Istio 1.5.&lt;/p&gt; &lt;p&gt;WebAssembly extensions provide two significant benefits over Istio Mixer:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;WebAssembly extensions live at the Envoy proxy level, so there is no single point of contention.&lt;/li&gt; &lt;li&gt;Extensions are deployed in a sandbox with resource constraints and a clearly defined API, providing better security and isolation.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Using the WebAssembly plugin with Envoy proxies&lt;/h3&gt; &lt;p&gt;Figure 2 shows the typical communication between WebAssembly components.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/wasm_comm.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/wasm_comm.png?itok=R5H54z2B" width="844" height="870" alt="A diagram showing how WebAssembly allows multiple services to communicate." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. WebAssembly allows multiple services to communicate. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;WebAssembly, as a format for extending Envoy, has been incorporated into Istio using the &lt;code&gt;ServiceMeshExtension&lt;/code&gt; API. This API specifies how to package and deploy a WebAssembly plugin to be used alongside Istio’s Envoy proxies. It is possible to deploy extensions to all proxies in the mesh, to specific namespaces, or to individual workloads using the &lt;code&gt;workloadSelector&lt;/code&gt; field. Consider the following sample &lt;code&gt;ServiceMeshExtension&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: maistra.io/v1alpha1 kind: ServiceMeshExtension metadata: name: header-append spec: config: custom-header: test image: quay.io/maistra-dev/header-append-filter:2.1 phase: PostAuthZ priority: 1000 workloadSelector: labels: app: httpbin &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Custom WebAssembly extensions&lt;/h2&gt; &lt;p&gt;There are two steps to writing and deploying a &lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html-single/service_mesh/index#ossm-extensions"&gt;custom WebAssembly extension&lt;/a&gt; to OpenShift Service Mesh:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Use the SDK and WebAssembly for the Proxies API to write the custom extension in a supported language.&lt;/li&gt; &lt;li&gt;Compile the WebAssembly extension into a &lt;code&gt;.wasm&lt;/code&gt; file containing the bytecode and provide a &lt;code&gt;Manifest.yaml&lt;/code&gt; in the container image's root folder.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;The 3scale WebAssembly extension&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/3scale/threescale-wasm-auth/"&gt;Proxy-WASM Authorization extension using 3scale&lt;/a&gt; (&lt;code&gt;threescale-wasm-auth&lt;/code&gt;) module is a Proxy-Wasm extension that authorizes HTTP requests made to 3scale. The extension is part of OpenShift Service Mesh 2.1 and, unlike custom WebAssembly extensions, is completely supported by Red Hat. The &lt;code&gt;threescale-wasm-auth&lt;/code&gt; module is designed to be fully compatible with all implementations of the Proxy-WASM ABI specification. The module has been thoroughly tested and is supported to work with the Envoy reverse proxy. Figure 3 shows typical communication through the extension.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/wasm-envoy.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/wasm-envoy.png?itok=LTiqJlyW" width="1152" height="816" alt="The Proxy-Wasm extension communicates through Envoy." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. The Proxy-Wasm extension communicates through Envoy. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The 3scale WebAssembly extension supports the following activities:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Connect to a 3scale Software-as-a-Service (SaaS), &lt;a href="https://developers.redhat.com/products/red-hat-openshift-api-management/getting-started"&gt;Red Hat OpenShift API Management&lt;/a&gt;, or a 3scale self-managed tenant for a specific provider account.&lt;/li&gt; &lt;li&gt;Configure one or many services to manage within the specified provider account.&lt;/li&gt; &lt;li&gt;Provide mapping rules to apply for each service.&lt;/li&gt; &lt;li&gt;Provide a security pattern to be used by the services. All supported 3scale patterns—API key, application identifier/key pair, and Open ID Connect (OIDC)—can be used.&lt;/li&gt; &lt;li&gt;As with &lt;a href="https://github.com/3scale/APIcast"&gt;APIcast&lt;/a&gt;, authorize and report each request to 3scale's backend URL through an API request to the Service Management API's AuthRep operation.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;How to configure a 3scale WebAssembly extension&lt;/h2&gt; &lt;p&gt;The 3scale WebAssembly configuration is provided by the &lt;code&gt;ServiceEntry&lt;/code&gt; and &lt;code&gt;ServiceMeshExtension&lt;/code&gt; custom resources (CRs). In the next sections, I'll explain how to configure these custom resources.&lt;/p&gt; &lt;h3&gt;The ServiceEntry custom resource&lt;/h3&gt; &lt;p&gt;This custom resource lets you add entries to Istio’s internal service registry so that auto-discovered services in the mesh can access and route to manually specified services. For 3scale WebAssembly, two service entries are needed.&lt;/p&gt; &lt;p&gt;The first entry is used to connect to the 3scale provider account's admin URL, in order to access the service configurations. Here is an example configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: networking.istio.io/v1beta1 kind: ServiceEntry metadata: labels: app: '' name: system-entry spec: hosts: - 3scale-admin.apps.cluster-qtrg2.qtrg2.sandbox961.opentlc.com location: MESH_EXTERNAL ports: - name: https number: 80 protocol: HTTP resolution: DNS &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second entry is used to connect to the backend URL of the 3scale provider, in order to authorize each request. Here is an example configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: networking.istio.io/v1beta1 kind: ServiceEntry metadata: labels: app: '' name: system-entry spec: hosts: - backend-3scale.apps.cluster-qtrg2.qtrg2.sandbox961.opentlc.com location: MESH_EXTERNAL ports: - name: https number: 80 protocol: HTTP resolution: DNS &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;The ServiceMeshExtension custom resource&lt;/h3&gt; &lt;p&gt;This custom resource configures the WebAssembly module containing the 3scale provider and access token, list of services, mapping rules, authentication methods, backend URL, and service token. An example of this configuration follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: maistra.io/v1 kind: ServiceMeshExtension metadata: labels: app: '' name: threescale-wasm-extension namespace: bookinfo spec: config: api: v1 backend: extensions: - no_body name: backend upstream: name: &gt;- outbound|80||backend-3scale.apps.cluster-qtrg2.qtrg2.sandbox961.opentlc.com timeout: 5000 url: 'http://backend-3scale.apps.cluster-qtrg2.qtrg2.sandbox961.opentlc.com' services: - authorities: - '*' credentials: app_id: - header: keys: - app_id - query_string: keys: - app_id app_key: - header: keys: - app_key - query_string: keys: - app_key user_key: - query_string: keys: - user_key - header: keys: - user_key id: '3' mapping_rules: - method: GET pattern: / usages: - delta: 1 name: hits token: 43a5db7bf61bdf55a67ff54edbee7c579b6a2956146cfef2179486561476abe0 system: name: system token: ose6aAdE777Dz0zb upstream: name: &gt;- outbound|80||3scale-admin.apps.cluster-qtrg2.qtrg2.sandbox961.opentlc.com timeout: 5000 url: 'http://3scale-admin.apps.cluster-qtrg2.qtrg2.sandbox961.opentlc.com' image: 'quay.io/3scale/threescale-wasm-auth:qe' phase: PostAuthZ priority: 100 workloadSelector: labels: app: productpage &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The service configuration contains the service ID, which is the unique ID of the service in 3scale. Also, in this release, the service configuration is provided in the &lt;code&gt;ServiceMeshExtension&lt;/code&gt; configuration. There are plans to fetch this configuration from the provider configuration in future releases.&lt;/p&gt; &lt;h2&gt;Watch a video demonstration&lt;/h2&gt; &lt;p&gt;The following video demonstrates how to use the Red Hat OpenShift user interface, commands, and configuration files to configure a 3scale WebAssembly extension.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Learn more at Apidays Paris (virtual event)&lt;/h2&gt; &lt;p&gt;Apidays is the leading industry tech and business series of conferences about APIs and the programmable economy. From December 7 through 9, 2021, &lt;a href="https://www.apidays.global/paris/"&gt;Apidays LIVE Paris - APIs and the Future of Software&lt;/a&gt; will host virtual workshops and roundtables with technical and business leaders who will explain how they create value with APIs, and how they aligned technical capabilities to make it happen.&lt;/p&gt; &lt;p&gt;&lt;a href="https://hopin.com/events/apidays-live-paris-2021"&gt;Registration&lt;/a&gt; at Apidays Paris is free, and you can join both of the Red Hat sessions at the event:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Tuesday, December 7, 2021 at 3:40 PM CET&lt;/strong&gt;: Author &lt;a href="mailto:sjayanti@redhat.com"&gt;Satya Jayanti&lt;/a&gt; will present on Istio and API management integration with 3scale WebAssembly extensions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wednesday, December 8, 2021 at 2:25 pm CET&lt;/strong&gt;: Author &lt;a href="mailto:hguerrero@redhat.com" target="_blank"&gt;Hugo Guerrero Olivares&lt;/a&gt; will present on getting started with event-driven APIs.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Red Hat is a sponsor of the conference, and our experts are available in our sponsor booth throughout the event to answer questions on API management, integration, open banking, and cloud-native development at Red Hat.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article introduced the benefits of WebAssembly and how to use it to create Envoy HTTP filter extensions for OpenShift Service Mesh. In particular, I introduced the 3scale WebAssembly authorization extension and showed you an example configuration.&lt;/p&gt; &lt;p&gt;To learn more about the 3scale WebAssembly extension, see the &lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html-single/service_mesh/index#ossm-threescale-webassembly-module"&gt;threescale-webassembly-module&lt;/a&gt; documentation. You might also want to visit the following resources for more detail and explanation:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;My &lt;a href="https://youtu.be/ljGsjwakYns"&gt;video demonstration&lt;/a&gt; showing how to configure a 3scale WebAssembly extension in OpenShift.&lt;/li&gt; &lt;li&gt;The article, &lt;a href="https://developers.redhat.com/articles/2021/11/18/design-authorization-cache-envoy-proxy-using-webassembly"&gt;Design an authorization cache for Envoy proxy using WebAssembly&lt;/a&gt; (November 2021).&lt;/li&gt; &lt;li&gt;The article, &lt;a href="https://developers.redhat.com/articles/2021/11/25/how-we-implemented-authorization-cache-envoy-proxy"&gt;How we implemented an authorization cache for Envoy proxy&lt;/a&gt; (November 2021).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The 3scale WebAssembly extension eases the integration of OpenShift Service Mesh and 3scale, and it provides a standard way to inject 3scale API Management configurations into OpenShift Service Mesh for execution in a single data plane. As one of the first WebAssembly extensions to API Management platforms, this work pioneers an efficient integration with OpenShift Service Mesh and paves the way for further Envoy proxy integrations to 3scale. We hope you will join us at Apidays Paris to learn more about it!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/06/custom-webassembly-extensions-openshift-service-mesh" title="Custom WebAssembly extensions in OpenShift Service Mesh"&gt;Custom WebAssembly extensions in OpenShift Service Mesh&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Satya Jayanti</dc:creator><dc:date>2021-12-06T07:00:00Z</dc:date></entry><entry><title type="html">Configuring Podman for Quarkus Dev Services and Testcontainers on Linux</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-devservices-testcontainers-podman/" /><author><name>Michal Jurč</name></author><id>https://quarkus.io/blog/quarkus-devservices-testcontainers-podman/</id><updated>2021-12-06T00:00:00Z</updated><content type="html">Podman is a daemonless container engine for developing, managing, and running Containers on Linux systems. Since the release of version 3, Podman allows the user to run a service emulating a Docker API provided on a Unix socket. This makes it possible for Testcontainers and Quarkus Dev Services to be...</content><dc:creator>Michal Jurč</dc:creator></entry></feed>
